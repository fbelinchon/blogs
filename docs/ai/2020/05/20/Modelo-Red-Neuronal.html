<!DOCTYPE html>

<html>

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>
    Modelos simples a Redes Neuronales - My Blog
    
  </title>
  <!-- Mathjax Support -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  
  
  <meta name="description" content="Modelo simple">

  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="/blogs/assets/vendor/bootstrap/css/bootstrap.min.css">

  <link rel="stylesheet" href="/blogs/assets/vendor/fontawesome-free/css/all.min.css">

  <link rel="stylesheet" href="/blogs/assets/main.css">
  <link rel="canonical" href="https://fbelinchon.github.io/blogs/ai/2020/05/20/Modelo-Red-Neuronal.html">
  <link rel="alternate" type="application/rss+xml" title="My Blog" href="/blogs/feed.xml">

</head>


<body>

  <!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="/blogs/">My Blog</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      Menu
      <i class="fa fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="/blogs/">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/blogs/about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/blogs/posts">Posts</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/blogs/AI">AI</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/blogs/contact">Contact</a>
        </li>
      </ul>
    </div>
  </div>
</nav>


  <!-- Page Header -->

<header class="masthead" style="background-image: url('/blogs/blogs/img/posts/06.jpg')">
  
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-12 col-md-14 mx-auto">
          <div class="post-heading">
            <h1>Modelos simples a Redes Neuronales</h1>
            
            <h2 class="subheading">Entendiendo modelos simples de Redes Neuronales</h2>
            
            <span class="meta">Posted by
              <a href="#">Francisco Belinchón</a>
              on May 20, 2020 &middot; <span class="reading-time" title="Estimated read time">
  
   19 mins  read </span>

            </span>
          </div>
        </div>
      </div>
    </div>
  </header>

<div class="container">
    <div class="row">
      <div class="col-lg-12 col-md-14 mx-auto">

        <h1 id="modelo-simple">Modelo simple</h1>

<h2 id="objetivo">Objetivo.</h2>
<p>El objetivo del artículo es entender como se define un modelo de apredizaje. Nos centraremos 
en la definición de modelos de redes neuronales sencillas para entender los mecanismos básicos.
Las redes neuronales se denominan <strong>deep learning</strong> porque se basan en arquitecturas de varios niveles que se
conectan de forma encadenada hasta producir el resultado final.</p>

<h2 id="configuración-simple-red-neuronal-de-una-sola-capa">Configuración simple: red neuronal de una sola capa</h2>

<h3 id="perceptron">Perceptron</h3>
<p>El modelo más básico de red neuronal es el perceptrón. Está definido por una neurona que acepta una entrada con varios parámetros y produce una salida. Idealmente la salida es 0 o 1. Para calcular el resultado final se multiplica cada entrada por un peso que nos indica el aporte de esa entrada al resultado final.</p>

<p>Al sumatorio de todas esas operaciones se le compara con un valor constante para devolver un 1 si el sumatorio es mayor que ese valor o un 0  en caso contrario. Si nos damos cuenta tenemos los mismo elementos que describimos en el árticulo anterior.</p>
<ul>
  <li>Una entrada de datos.</li>
  <li>Una matriz (en este caso un vector) de pesos.</li>
  <li>Un parámetro de ajuste para indicar el umbral del cero que denominamos bias y que se añade a la multiplicación de ,los valores anteriores.</li>
  <li>El resultado de nuestro modelo. En este caso concreto sería 0 o 1.</li>
</ul>

<p>Este modelo lo podemos representar matemáticamente como:</p>

<div class="kdmath">$$
\hat{y}=W*x + b
$$</div>

<p><img src="/blogs/img/perceptron.png" alt="perceptron" /></p>
<blockquote>
  <p>Imagen perceptrón con cinco entradas tomada de Wikipedia</p>
</blockquote>

<p>Para ajustar la salida de un perceptrón que sería de 0 o 1 podemos indicar que el valor será un 1 si el resultado es positivo y cero en caso contrario.</p>

<p>El perceptrón es un modelo muy simple donde solamente tenemos una neurona y una sola capa. Como veremos más adelante las redes neuronales constan de cientos de neuronas en cada capa y de varias capas que se encadenan unas a otras. Es decir, la salida de una capa es la entrada a la siguiente.</p>

<h3 id="multiplicación-de-matrices">Multiplicación de matrices</h3>

<p>El ejemplo del perceptron se define matemáticamente como una multiplicación de vectores. Al multiplicar el vector entrada $\vec{x}$ por el vector $\vec{w}$ de los pesos tenemos las operaciones de multiplicación que hemos definido.</p>

<div class="kdmath">$$
\left[\begin{array}{ccc}x_1&amp; x_2 &amp; x_3\end{array}\right] \left[\begin{array}{ccc}w_1 \\ w_2 \\ w_3\end{array}\right] = x_1*w_1+x_2*w_2+x_3*w_3
$$</div>

<p>Si queremos añadir más neuronas a nuestro modelo necesitamos tratar con multiplicaciones de matrices. Imaginemos que se añade una nueva neurona. Tendrá el mismo número de entradas pero tendremos dos resultados de nuestro modelo, uno por cada neurona. Cada nueva columna de nuestra matriz de pesos representa una neurona con sus correspondietnes pesos que se aplican a cada entrada. Si lo expresamos como operaciones de matrices trandríamos lo siguiente.</p>

<div class="kdmath">$$
\left[\begin{array}{ccc}x_1&amp; x_2 &amp; x_3\end{array}\right] \left[\begin{array}{ccc}w_{1n1}&amp;w_{1n2} \\ w_{2n1} &amp; w_{2n2}\\ w_{3n1} &amp; w_{3n2}\end{array}\right] = \begin{cases} x_1*w_{1n1}+x_{2}*w_{2n1}+x_3*w_{3n1} \\ x_1*w_{1n2}+x_{2}*w_{2n2}+x_3*w_{3n2}\end{cases}
$$</div>

<p>Hemos numerado los índices de los pesos de forma que el primer elemento es el orden del peso y el segundo la neurona. Por ejemplo $w_{3n2}$ sería el peso que se aplica a la tercera entrada perteneciente a la segunda neurona. Normalmente en matrices se habla de filas y columnas (en ese orden). A partir de ahora utilizaremos esa nomenclatura donde el primer valor (fila) es el orden del peso y el segundo (columna) es la neurona, <span class="kdmath">$w_{32}$</span> (es el tercer parámetro de la segunda neurona)</p>

<p>Es importante tener claro unos conceptos básicos de multiplicación de matrices.</p>
<ul>
  <li>Para multiplicar dos matrices necesitamos que el número de columnas de la primera matriz sea igual al número de filas de la segunda matriz. Tiene sentido porque el número de parámetros de entrada tiene que ser igual al número de pesos que tiene cada neurona.</li>
  <li>La matriz resultado tendrá el mismo número de filas que la primera matriz y el mismo número de columnas de la segunda. También concuerda con lo que hemos explicado. Si tenemos como entrada un vector <span class="kdmath">$\vec{x} = \left[\begin{array}{ccc}x_1&amp; x_2 &amp; x_3\end{array}\right]$</span> (1 fila y 3 columnas) y nuestro modelo tiene dos neuronas (3 filas y 2 columnas) el resultado tendría una fila y dos columnas, una columnas para el resultado de cada neurona. Simplificando, el resultado de nuestro modelo sería igual al número de registros de entrada (en nuestro ejemplo uno) y tantas columnas como neuronas tenemos <span class="kdmath">$X(1,3) * W(3,2) = R(1,2)$</span></li>
  <li>
    <p>El orden de la multiplicación afecta al resultado. No es lo mismo multiplicar <span class="kdmath">$A*B$ que $B*A$</span>.</p>

    <blockquote>
      <p>Nota: Si <span class="kdmath">$A*B = R$</span> entonces <span class="kdmath">$B^T*A^T =R^T$</span>. Eso quiere decir que si cambiamos el orden de los factores de la multiplicación tenemos que utilizar las matrices traspuestas de cada uno de ellos y el resultado será la traspuesta de $A*B$. La traspuesta de una matriz se obtiene cambiando filas por columnas.</p>
    </blockquote>
  </li>
</ul>

<p>Para aclarar un poco más el tema podemos poner un ejemplo donde como entrada tenemos dos registros y nuestro modelo tiene 3 neuronas. <span class="kdmath">$X(2,3) * W(3,3) = R(2,3)$</span></p>

<div class="kdmath">$$
\left[\begin{array}{ccc}x_{11}&amp; x_{12} &amp; x_{13} \\ x_{21}&amp; x_{22} &amp; x_{23}\end{array}\right] * \left[\begin{array}{ccc}w_{11}&amp;w_{12} &amp; w_{13} \\ w_{21} &amp; w_{22} &amp; w_{23}\\ w_{31} &amp; w_{32}&amp; w_{33}\end{array}\right] = \left[\begin{array}{ccc} r_{11}  &amp; r_{12} &amp; r_{13}\\ r_{21}  &amp; r_{22} &amp; r_{23}\end{array}\right]
$$</div>

<div class="kdmath">$$
r_{11} = x_{11}*w_{11} + x_{12}*w_{21} + x_{13}*w_{31}
$$</div>

<div class="kdmath">$$
r_{12} = x_{11}*w_{12} + x_{12}*w_{22} + x_{13}*w_{32}
$$</div>

<div class="kdmath">$$
r_{13} = x_{11}*w_{13} + x_{12}*w_{23} + x_{13}*w_{33}
$$</div>

<div class="kdmath">$$
r_{21} = x_{21}*w_{11} + x_{22}*w_{22} + x_{23}*w_{31}
$$</div>

<div class="kdmath">$$
r_{22} = x_{21}*w_{12} + x_{22}*w_{22} + x_{23}*w_{32}
$$</div>

<div class="kdmath">$$
r_{23} = x_{21}*w_{13} + x_{22}*w_{23} + x_{23}*w_{33}
$$</div>

<p>La primera fila del resultado es el valor de cada una de las tres neuronas para el primer registro de entrada. La segunda fila corresponde a los valores de cada neurona para el segundo registro.</p>

<p>Conocer las dimensiones de la salida de nuestro modelo es esencial para poder incorporar más niveles a nuestro modelo.</p>

<h2 id="red-neuronal-modelo-multicapa">Red neuronal: modelo multicapa</h2>

<h3 id="añadimos-nueva-capa">Añadimos nueva capa</h3>

<p>Las redes neuronales con una sola capa tienen limitada su capacidad de predicción. El objetivo es convertir nuestro perceptron en una red neuronal con multiples capas. Esto es lo que conocemos como Deep Learning.</p>

<p>Desde el punto de vista de arquitectura para añadir nueva capa simplemente tomamos la salida de la primera capa como entrada de la segunda. Se trata de encadenar varias capas para generar un modelo más complejo que permita más capacidad de análisis de patrones y relaciones.</p>

<p>Cada capa es básicamente una matriz de pesos más el temino bias que se suma posteriormente. La dimensión de la matriz para nuestra segunda capa depende del número de neuronas de la capa anterior (ya definida) y el número de neuronas que queremos en la segunda capa. Sabemos que para multiplicar matrices las columnas de la primera capa tienen que ser iguales a las filas de la segunda. Para añadir una nueva capa tenemos que definir la matrix de pesos que tendrá como número de filas el número de neuronas de la capa anterior y el número de columnas igual al número de neuronas de la nueva capa.</p>

<p>Desde un punto de vista más matemático añadir una nueva capa representa una composición de funciones. Una composiciñon de funciones significa que la salida de una función es la entrada de la siguiente lo que encaja perfectamente con lo descripto en la parte de arquitectura de nuestro modelo.</p>

<p>Supongamos que tenemos dos funciones <span class="kdmath">$f(x) = W_1*x + b_1$</span> y una función <span class="kdmath">$g(x)=W_2*x + b_2$</span>. El resultado final de la composición de estas dos funciones sería:
<span class="kdmath">$m(x)=g(f(x))$</span>. Esto quiere decir que la variable <span class="kdmath">$x$</span> de la función <span class="kdmath">$g(x)$</span> se sustituye por la propia función <span class="kdmath">$f(x)$</span>.</p>

<div class="kdmath">$$
m(x)=g(f(x))=g(W_1*x + b_1)= W_2*(W_1*x + b_1) + b_2
$$</div>

<p>Si <span class="kdmath">$m(x)$</span> es nuestro modelo, los datos de entrada serían la <span class="kdmath">$x$</span>. Vemos que en la función desarrollada aparecen los pesos de nuestras dos capas y sus correspondientes terminos bias. Nuestro modelo tendrá que ajustar los pesos de las matrices <span class="kdmath">$W_1$</span> y <span class="kdmath">$W_2$</span> y los términos <span class="kdmath">$b_1$</span> y <span class="kdmath">$b_2$</span> para que el error cometido por nuestro modelo con respecto al valor real de losnuestros datos de entrenamiento sea el menor posible.</p>

<h3 id="funciones-de-activación">Funciones de activación.</h3>

<p>Lo primero que tenemos que decir es que el modelo descrito anteriomente no funciona. Una composición de dos funciones lineales es otra función linear. Eso significa que nuestro modelo con dos capas se comporta exactamente igual que un modelo de una sola capa. No aumentamos la capacidad de aprendizaje al añadir nuevas capas</p>

<p>Sin entrar en mucho detalle desarrollamos la función anterior.</p>

<div class="kdmath">$$
m(x)=g(f(x))=g(W_1*x + b_1)= W_2*(W_1*x + b_1) + b_2 = W_2*W_1*x + W_2*b_1 + b_2 = W*x + b
$$</div>

<blockquote>
  <p>Si aplicamos las reglas del cálculo de matrices podrías comprobar <span class="kdmath">$W_2*W_1$</span> sería una matriz de pesos similar la capa uno y que <span class="kdmath">$W_2*b_1$</span> sería un vector de las mismas dimensiones que <span class="kdmath">$b_2$</span> y al sumarlo se comportaría como un nuevo termino bias (sesgo).</p>
</blockquote>

<p>Para evitar este problema se aplica una función no linear al final de cada capa. De esta forma se rompe la linearidad y podemos añadir más capas de forma efectiva. Cada nueva capa significa nuevos pesos y nuevos terminos bias (sesgos) que el modelo tiene que ajustar. De esta forma aumentamos la capacidad de aprendizaje del modelo al añadir nuevas capas.</p>

<p>Es importante tener en cuenta que al aplicar una función de activación no modificamos ni la dimensión de la matriz resultante ni añadimos más parámetros al modelo.</p>

<p>Las funciones de activación más comunes son:</p>
<ul>
  <li>Logistic (sigmoid): <span class="kdmath">${\displaystyle S(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{e^{x}+1}}}$</span></li>
</ul>

<p><img src="/blogs/img/sigmoid.png" alt="logistic" /></p>

<ul>
  <li>tangente hiperbolica: <span class="kdmath">${\displaystyle \tanh x={\cfrac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}}$</span></li>
</ul>

<p><img src="/blogs/img/tanh.png" alt="tanh" /></p>

<ul>
  <li>Relu: <span class="kdmath">$\displaystyle f(x) =\max(0,x)$</span></li>
</ul>

<p><img src="/blogs/img/relu.png" alt="Relu" /></p>

<p>Inicialmente la función logistic se utilizaba con mucha frecuencia como función de activación. Actualmente Relu es la función más utilizada principalmente con configuraciones de redes neuronales sencillas. En redes neuronales más complejas como redes convolucionales o recurrentes también se utiliza logistic y tanh.</p>

<p>### Modelo multicapa</p>

<p>Ahora que entendemos como podemos añadir nuevas capas a nuestro modelo vamos a definir más en detalle una arquitectura típica. Tenemos tres tipos de capas.</p>
<ul>
  <li>Capa de entrada (input layer): no es una capa en si, sino la entrada al modelo. Esta capa representa la información de entrenamiento. Las columnas representan cada una de las características de nuestra información y las filas el número de ejemplos que vamos a pasar al modelo.</li>
  <li>Capa de salida (output layer): capa final que devuelve el resultado del modelo. El número de neuronas depende del tipo de problema que estamos abordando. Si el modelo predice un valor único tendremos una sola neurona con una salida para cada registro de entrada. Si estamos realizando una clasificación tendremos tantas salidas como categorías de clasificación, lo que supondría una matriz de resultados.</li>
  <li>Capa oculta (hidden layer): capas intermedias con la configuración que hemos descrito en el apartado anterior. Se componen de una función lineal seguido de una función de activación. La definición de la matriz de estas capas depende del número de neuronas de la capa anterior y del número de neuronas que queremos en la actual.</li>
</ul>

<h3 id="ejemplo-práctico">Ejemplo Práctico</h3>

<p>Vamos a ver un ejemplo sencillo.</p>
<blockquote>
  <p>Somos una inmoviliaría que queremos informar a nuestros clientes del precio estimado de venta de su vivienda de forma inmediata. De operaciones anteriores tenemos información de 2.000 viviendas con sus características y sus precios reales de venta. De cada vivienda tenemos 45 caracterísitcas que nos definen el inmueble (planta, metros cuadrados, número de habitaciones, localidad, barrio etc).</p>
</blockquote>

<p>Tomamos la decisión de crear un modelo de red neuronal con cuatro capas. La primera capa representa la entrada y decidimos pasar la información de entrenamiento a nuestro modelo en paquetes de 32 viviendas. añadimos dos capas ocultas donde la primera capa tendrá 200 neuronas y la segunda 100. Como queremos predecir un valor único que será el precio estimado de venta la capa de salida tendrá una única neurona.</p>

<p>El esquema de nuestra red neuronal sería de esta manera.</p>

<p><img src="/blogs/img/ejemplo_rn.jpg" alt="red neuronal" title="perceptron" width="100%" /></p>

<p>Vamos a analizar los parámetros y las dimensiones de las matrices en cada capa para conocer mejor el funcionamiento interno.</p>

<ul>
  <li>Capa de entrada (input layer): es la más sencilla de analizar. Si cada vivienda tiene 45 características y al modelo le pasamos 32 viviendas la entrada al modelo es una matriz input(32,45). Tenemos 32 filas y 45 columnas.</li>
  <li>
    <p>Primera capa oculta (hidden layer 1): Esta es una capa propiamente dicha de nuestro modelo por lo que tenemos una matriz de pesos y un vector de terminos bias. La matriz de entrada se multiplica por la matriz de pesos y se le suma el término bias. Tanto la matriz de pesos como el vector bias de suma son parámetros de esta capa. Tenemos 9200 parámetros</p>

    <ul>
      <li>Matriz de pesos: <span class="kdmath">$W_1(45,200) = 9000$</span> parámetros</li>
      <li>Bias: <span class="kdmath">$b_1(200) = 200$</span> parámetros</li>
    </ul>
  </li>
  <li>
    <p>Segunda capa oculta (hidden layer 2): Toma como entrada la salida de la capa anterior. El número de filas corresponde al número de neuronas de la capa anterior. Las columnas corresponden al número de neuronas escogido para esta capa. Tenemos 20100 parámetros</p>

    <ul>
      <li>Matriz de pesos: <span class="kdmath">$W_1(200,100) = 20000$</span> parámetros</li>
      <li>Bias: <span class="kdmath">$b_1(100) = 100$</span> parámetros</li>
    </ul>
  </li>
  <li>
    <p>Capa de salida (output layer): Toma como entrada la salida de la capa anterior. El número de filas corresponde al número de neuronas de la capa anterior. Las columnas corresponden al número de nueronas decidio para esta capa, en este caso una única columna. Tenemos 101 parámetros</p>

    <ul>
      <li>Matriz de pesos: <span class="kdmath">$W_1(100,1) = 100$</span> parámetros</li>
      <li>Bias: <span class="kdmath">$b_1(1) = 1$</span> parámetros</li>
    </ul>
  </li>
</ul>

<p>En total el modelo tiene x parámetros que tiene que ajustar en la fase de entrenamiento. Una vez entrenado nuestro modelo podemos utilizarlo para predecir el coste estimado de venta de nuestra vivienda. Un modelo ya entrenado y listo para su uso se compone de dos partes.</p>
<ul>
  <li>Arquitectura: descripción de las diferentes capas de nuestro modelo.</li>
  <li>Parámetros: los valores que se han ajustado en la fase de entrenamiento.</li>
</ul>

<h1 id="conclusión">Conclusión</h1>

<p>Un modelo de red neuronal se compone de varias capas que constituyen la arquitectura del modelo. Cada capa consiste en una función lineal más una función de activación.</p>

<p>La función lineal es del tipo <span class="kdmath">$f(x) = W*x +b$</span> y consiste en la matriz de pesos <span class="kdmath">$W$</span> y el término de sesgo o bias <span class="kdmath">$b$</span>. La matriz de pesos y el término bias representan los parámetros de la capa que el modelo tiene que ajustar en la fase de entrenamiento.</p>

<p>La función de activación cumple la misión de evitar la linearidad al componer varias capas. No afecta ni a las dimensiones de la salida de la capa ni tampoco genera nuevos parámetros. Simplemente es una función que se aplica a la salida de la función lineal. Relu es una de las funciones de activación más utilizada. Simplemente devuelve un cero si el valor es negativo y el propio valor si es positivo.</p>

<p>El tipo de modelo que hemos explicado es el más sencillo y se denomina fully-connected neural network. Existen otros tipos de capas con configraciones más complejas pero todas comparten un patrón de capas encadenadas.</p>

<p>En el siguiente árticulo hablaremos del proceso de entrenamiento  y como conseguir modelos que nos pemitan predecir valores o inferir características a partir de datos nuevos (distintos a los utiilizados para entranar le modelo).</p>


        <hr />

        <div class="clearfix">

          
          <a class="btn btn-primary float-left" href="/blogs/ai/2020/05/19/Introduccion.html" data-toggle="tooltip" data-placement="top" title="Introducción a Redes Neuronales">&larr; Anteriores<span class="d-none d-md-inline">
              Post</span></a>
          
          

        </div>

      </div>
    </div>
  </div>


  <!-- Footer -->

<hr>

<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <ul class="list-inline text-center">
          
          <li class="list-inline-item">
            <a href="mailto:fbelinchon@gmail.com">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="far fa-envelope fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li class="list-inline-item">
            <a href="https://twitter.com/fbelinchon">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li class="list-inline-item">
            <a href="https://www.facebook.com/StartBootstrap">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/fbelinchon">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-github fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="copyright text-muted">Copyright &copy; Francisco Belinchón 2020</p>
      </div>
    </div>
  </div>
</footer>


  <script src="/blogs/assets/vendor/jquery/jquery.min.js"></script>
<script src="/blogs/assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="/blogs/assets/vendor/startbootstrap-clean-blog/js/clean-blog.min.js"></script>

<script src="/blogs/assets/scripts.js"></script>





  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX-X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-XXXXXXXXX-X');
</script>



</body>

</html>
